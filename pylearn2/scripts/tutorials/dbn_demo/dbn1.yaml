# This file shows how to train a binary RBM on MNIST by viewing it as a single layer DBM.
# The hyperparameters in this file aren't especially great; they're mostly chosen to demonstrate
# the interface. Feel free to suggest better hyperparameters!
!obj:pylearn2.train.Train {
    # For this example, we will train on a binarized version of MNIST.
    # We binarize by drawing samples--if an MNIST pixel is set to 0.9,
    # we make binary pixel that is 1 with probability 0.9. We redo the
    # sampling every time the example is presented to the learning
    # algorithm.
    # In pylearn2, we do this by making a Binarizer dataset. The Binarizer
    # is a dataset that can draw samples like this in terms of any
    # input dataset with values in [0,1].
    dataset: &data !obj:pylearn2.datasets.binarizer.Binarizer {
        # We use the "raw" tag to specify the underlying dataset defining
        # the sampling probabilities should be MNIST.
        raw: &raw_train !obj:pylearn2.datasets.mnist.MNIST {
            which_set: "train",
            one_hot: 1,
            start: 0,
            stop: %(train_stop)i
        }
    },
    model: !obj:pylearn2.models.dbn.DBN {
        lower_model: !pkl: "${PYLEARN2_ROOT}/scripts/tutorials/dbn_demo/rbm.pkl",
        rbm: !obj:pylearn2.models.dbm.RBM {
            batch_size: &batch_size %(batch_size)i,
            # 1 mean field iteration reaches convergence in the RBM
            niter: 1,
            # The visible layer of this RBM is just a binary vector
            # (as opposed to a binary image for convolutional models,
            # a Gaussian distributed vector, etc.)
            visible_layer: !obj:pylearn2.models.dbm.BinaryVector {
                nvis: %(nvis)i
            },
            hidden_layer:
                # This RBM has one hidden layer, consisting of a binary vector.
                # Optionally, one can do max pooling on top of this vector, but
                # here we don't, by setting pool_size = 1.
                !obj:pylearn2.models.dbm.BinaryVectorMaxPool {
                    # Every layer in the DBM must have a layer_name field.
                    # These are used to generate unique names of monitoring
                    # channels associated with the different layers.
                    layer_name: 'h',
                    # The detector layer is the portion of this layer that
                    # precedes the pooling. We control its size with this
                    # argument.
                    detector_layer_dim: %(detector_layer_dim)i,
                    pool_size: 1,
                    # We initialize the weights by drawing them from W_ij ~ U(-irange, irange)
                    irange: .05,
                    # We initialize all the biases of the hidden units to a negative
                    # number. This helps to learn a sparse representation.
                    init_bias: 0.,
                }
        },
    },
    # We train the model using stochastic gradient descent.
    # One benefit of using pylearn2 is that we can use the exact same piece of
    # code to train a DBM as to train an MLP. The interface that SGD uses to get
    # the gradient of the cost function from an MLP can also get the *approximate*
    # gradient from a DBM.
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
               # We initialize the learning rate and momentum here. Down below
               # we can control the way they decay with various callbacks.
               learning_rate: 1e-1,
               # Compute new model parameters using SGD + Momentum
               learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
                   init_momentum: 0.5,
               },
               # These arguments say to compute the monitoring channels on 10 batches
               # of the training set.
               monitoring_batches: %(monitoring_batches)i,
               monitoring_dataset : *data,
               # The SumOfCosts allows us to add together a few terms to make a complicated
               # cost function.
               cost : !obj:pylearn2.costs.cost.SumOfCosts {
                costs: [
                        # The cost function is contrastive divergence.
                        # For the RBM, the variational approximation is exact.
                        !obj:pylearn2.costs.dbm.VariationalCD {
                            num_gibbs_steps: 1
                        },
                       ],
           },
           # We tell the RBM to train for x epochs
           termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter { max_epochs: %(max_epochs)i },
           update_callbacks: [
                # This callback makes the learning rate shrink by dividing it by decay_factor after
                # each sgd step.
                !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {
                        decay_factor: 1.00001,
                        min_lr:       0.001
                }
           ]
        },
    extensions: [
            # This callback makes the momentum grow to 0.9 linearly. It starts
            # growing at epoch 5 and finishes growing at epoch 6.
            !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {
                final_momentum: .9,
                start: 5,
                saturate: 6
            },
    ],
    save_path: "%(save_path)s/dbn1.pkl",
    # This says to save it every epoch
    save_freq : 1
}